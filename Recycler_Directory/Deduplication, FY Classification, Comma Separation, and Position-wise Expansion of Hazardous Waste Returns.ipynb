{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed87c356-204a-4285-bcc5-9a2ec7aed438",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "To clean, de-duplicate, classify, and structurally expand Hazardous Waste Annual Return data for accurate analysis and reporting, \n",
    "while preserving fiscal-year logic and authorization-wise details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5110df0-f171-4d92-9738-3711d9126e75",
   "metadata": {},
   "source": [
    "#### Brief Description\n",
    "\n",
    "This script is applied to the Hazardous Waste Generating Industries – Annual Return file and performs two sequential processing tasks, \n",
    "producing two structured Excel outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c5a94-b453-401e-998e-d75098440533",
   "metadata": {},
   "source": [
    "#### Final Outputs\n",
    "\n",
    "This script generates two Excel files:\n",
    "> Task 1 Output\n",
    "\n",
    "* Cleaned, de-duplicated, FY-segregated datasets\n",
    "* Suitable for validation, compliance checks, and reporting\n",
    "\n",
    "> Task 2 Output\n",
    "\n",
    "* Authorization-wise expanded datasets\n",
    "* Suitable for aggregation, analysis, and database ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cea3405-2b33-4706-aa3d-0c9f30336970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing...\n",
      "Loading 'D:/Techknowgreen/Rutuja_mam/12noV_Hazardous/Annual_Return_24-25.xlsx' ...\n",
      "Detected Unique ID column: 'Unique id' (will be ignored during duplicate checks).\n",
      "Using Submitted-on column: 'Submited on' (Excel B)\n",
      "Using Submission-Year column: 'Submission year' (Excel AH)\n",
      "Parsed 18310/18310 Submitted-on values.\n",
      "Number of columns used for comparisons (original headers minus Unique id & Submitted on): 62\n",
      "Submission Year Variants rows found: 106\n",
      "Complete duplicate rows found: 5\n",
      "Rows in Main FY 24-25 (before display fix): 8954\n",
      "Rows in Main Outside FY (before display fix): 9245\n",
      "Writing Task 1 workbook to 'D:/Techknowgreen/Rutuja_mam/12noV_Hazardous/task1_output_C_fix.xlsx' ...\n",
      "Task 1 complete.\n",
      "Columns to position-wise split: ['Type of waste dispatched', 'Quantity of waste dispatched', 'Unit dispatched', 'Dispatched to', 'Dispatched to facility']\n",
      "Creating Task 2 workbook: 'D:/Techknowgreen/Rutuja_mam/12noV_Hazardous/task2_output_C_fix.xlsx' ...\n",
      "Task 2 complete. Files written:\n",
      " - D:/Techknowgreen/Rutuja_mam/12noV_Hazardous/task1_output_C_fix.xlsx\n",
      " - D:/Techknowgreen/Rutuja_mam/12noV_Hazardous/task2_output_C_fix.xlsx\n",
      "\n",
      "Sample complete-duplicate Unique IDs (moved to Complete Duplicates):\n",
      "['MPCB-HW_ANNUAL_RETURN-0000053868', 'MPCB-HW_ANNUAL_RETURN-0000044129', 'MPCB-HW_ANNUAL_RETURN-0000060548', 'MPCB-HW_ANNUAL_RETURN-0000043580', 'MPCB-HW_ANNUAL_RETURN-0000058793']\n",
      "\n",
      "Sample Year-Variant Unique IDs (moved to Year Variants):\n",
      "['MPCB-HW_ANNUAL_RETURN-0000051661', 'MPCB-HW_ANNUAL_RETURN-0000051662', 'MPCB-HW_ANNUAL_RETURN-0000060092', 'MPCB-HW_ANNUAL_RETURN-0000060093', 'MPCB-HW_ANNUAL_RETURN-0000043951']\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Jupyter-ready full script — comparison columns chosen from original header list,\n",
    "so helper columns won't break duplicate / variant detection.\n",
    "\n",
    "Paste entire cell into Jupyter and run. Edit input_path, out1, out2 at bottom.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl.utils import column_index_from_string\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def find_unique_id_col(cols):\n",
    "    for c in cols:\n",
    "        if isinstance(c, str) and ('unique' in c.lower() and 'id' in c.lower()):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def colname_by_excel_letter(df, letter):\n",
    "    idx = column_index_from_string(letter) - 1\n",
    "    if idx < 0 or idx >= len(df.columns):\n",
    "        raise IndexError(f\"Excel column {letter} -> index {idx} out of range (len cols={len(df.columns)}).\")\n",
    "    return df.columns[idx]\n",
    "\n",
    "def normalize_for_compare(df):\n",
    "    out = df.copy()\n",
    "    for c in out.select_dtypes(include=['object']).columns:\n",
    "        out[c] = out[c].astype(str).str.strip()\n",
    "        out.loc[out[c].isin(['nan', 'None', 'NA', '']), c] = pd.NA\n",
    "    return out\n",
    "\n",
    "def parse_submitted_on_value(val):\n",
    "    raw = val\n",
    "    if pd.isna(val):\n",
    "        return (pd.NaT, None, val)\n",
    "    s = str(val).strip()\n",
    "    if s == '':\n",
    "        return (pd.NaT, None, val)\n",
    "    s = re.sub(r'[\\x00-\\x1f]+', ' ', s).strip()\n",
    "    # remove millisecond chunk before AM/PM like \":637PM\"\n",
    "    s = re.sub(r':\\d{1,6}(?=\\s*[AP]M$)', '', s, flags=re.IGNORECASE)\n",
    "    # ensure space before AM/PM if missing\n",
    "    s = re.sub(r'(?i)(\\d{1,2}:\\d{2}:\\d{2})(AM|PM)$', r'\\1 \\2', s)\n",
    "    dt = pd.to_datetime(s, errors='coerce')\n",
    "    if pd.isna(dt):\n",
    "        fmts = [\n",
    "            \"%b %d %Y %I:%M:%S %p\",\n",
    "            \"%d-%b-%Y %H:%M:%S\",\n",
    "            \"%Y-%m-%d %H:%M:%S\",\n",
    "            \"%d/%m/%Y %H:%M:%S\",\n",
    "        ]\n",
    "        for f in fmts:\n",
    "            try:\n",
    "                dt = pd.to_datetime(datetime.strptime(s, f))\n",
    "                break\n",
    "            except Exception:\n",
    "                dt = pd.NaT\n",
    "    display = dt.strftime(\"%Y-%m-%d %H:%M:%S\") if not pd.isna(dt) else s\n",
    "    return (dt, display, raw)\n",
    "\n",
    "def explode_columns_positionwise(df, cols_to_explode):\n",
    "    out_rows = []\n",
    "    if df.shape[0] == 0:\n",
    "        return pd.DataFrame(columns=df.columns)\n",
    "    for _, row in df.iterrows():\n",
    "        splits = {}\n",
    "        max_len = 0\n",
    "        for col in cols_to_explode:\n",
    "            val = row.get(col, \"\")\n",
    "            if pd.isna(val) or str(val).strip() == \"\":\n",
    "                parts = []\n",
    "            else:\n",
    "                parts = [x.strip() for x in str(val).split(',') if x.strip() != \"\"]\n",
    "            splits[col] = parts\n",
    "            max_len = max(max_len, len(parts))\n",
    "        if max_len == 0:\n",
    "            out_rows.append(row.to_dict())\n",
    "            continue\n",
    "        for i in range(max_len):\n",
    "            new_row = row.copy()\n",
    "            for col in cols_to_explode:\n",
    "                vals = splits[col]\n",
    "                new_row[col] = vals[i] if i < len(vals) else pd.NA\n",
    "            out_rows.append(new_row.to_dict())\n",
    "    return pd.DataFrame(out_rows, columns=df.columns)\n",
    "\n",
    "def make_safe_sheet_name(name, existing):\n",
    "    MAX = 31\n",
    "    base = name[:MAX]\n",
    "    candidate = base\n",
    "    i = 1\n",
    "    while candidate in existing:\n",
    "        suffix = f\"_{i}\"\n",
    "        candidate = (base[:MAX - len(suffix)]) + suffix\n",
    "        i += 1\n",
    "        if i > 999:\n",
    "            raise RuntimeError(\"Too many duplicate sheet names\")\n",
    "    existing.add(candidate)\n",
    "    return candidate\n",
    "\n",
    "def main(input_path, out_task1_path, out_task2_path):\n",
    "    print(f\"Loading '{input_path}' ...\")\n",
    "    df = pd.read_excel(input_path, sheet_name=0, header=0, engine='openpyxl')\n",
    "    original_cols = list(df.columns)  # capture original headers immediately\n",
    "\n",
    "    unique_col = find_unique_id_col(original_cols)\n",
    "    if unique_col:\n",
    "        print(f\"Detected Unique ID column: '{unique_col}' (will be ignored during duplicate checks).\")\n",
    "    else:\n",
    "        print(\"No Unique id column detected automatically. All columns (except Submitted on) will be used for comparisons.\")\n",
    "\n",
    "    try:\n",
    "        submitted_on_col = colname_by_excel_letter(df, 'B')\n",
    "        submission_year_col = colname_by_excel_letter(df, 'AH')\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to map Excel letters to columns: {e}\")\n",
    "\n",
    "    print(f\"Using Submitted-on column: '{submitted_on_col}' (Excel B)\")\n",
    "    print(f\"Using Submission-Year column: '{submission_year_col}' (Excel AH)\")\n",
    "\n",
    "    # parse Submitted-on robustly\n",
    "    raw_submitted = df[submitted_on_col].copy()\n",
    "    parsed_list = []\n",
    "    display_list = []\n",
    "    parsed_count = 0\n",
    "    for v in raw_submitted:\n",
    "        dt, disp, raw = parse_submitted_on_value(v)\n",
    "        parsed_list.append(dt)\n",
    "        display_list.append(disp)\n",
    "        if not pd.isna(dt):\n",
    "            parsed_count += 1\n",
    "    print(f\"Parsed {parsed_count}/{len(raw_submitted)} Submitted-on values.\")\n",
    "\n",
    "    parsed_col = '_parsed_submitted_on'\n",
    "    df[parsed_col] = pd.to_datetime(pd.Series(parsed_list))\n",
    "    df['_display_submitted_on'] = display_list\n",
    "\n",
    "    # mark original index for stable selection of \"first\" duplicate\n",
    "    df['_orig_index'] = range(len(df))\n",
    "\n",
    "    # --- Build comparison column lists FROM original_cols so helper cols aren't included ---\n",
    "    # exclude Unique id and Submitted on\n",
    "    exclude_for_comp = [submitted_on_col, '_orig_index']\n",
    "    if unique_col:\n",
    "        exclude_for_comp.append(unique_col)\n",
    "\n",
    "    cols_for_comp = [c for c in original_cols if c not in exclude_for_comp]\n",
    "    # cols_for_comp now contains only original columns used for comparisons (submission_year_col still included)\n",
    "    print(f\"Number of columns used for comparisons (original headers minus Unique id & Submitted on): {len(cols_for_comp)}\")\n",
    "\n",
    "    # normalized df for comparisons\n",
    "    df_for_comp_norm = normalize_for_compare(df[cols_for_comp].copy())\n",
    "\n",
    "    # ----- Submission Year Variants (identical except Submission Year) -----\n",
    "    # Build key excluding submission_year_col (and submitted_on is already excluded)\n",
    "    cols_except_year = [c for c in cols_for_comp if c != submission_year_col]\n",
    "    # create tuple key for every row\n",
    "    df['_key_except_year'] = df_for_comp_norm[cols_except_year].astype(object).apply(lambda row: tuple(row.values), axis=1)\n",
    "    # group\n",
    "    groups_except_year = df.groupby('_key_except_year')\n",
    "\n",
    "    variant_indices = []\n",
    "    for k, g in groups_except_year:\n",
    "        unique_years = g[submission_year_col].astype(str).dropna().unique()\n",
    "        if len(unique_years) > 1:\n",
    "            variant_indices.extend(g.index.tolist())\n",
    "\n",
    "    variant_df = df.loc[variant_indices].copy().drop(columns=['_key_except_year'])\n",
    "    print(f\"Submission Year Variants rows found: {len(variant_df)}\")\n",
    "\n",
    "    # Remove variants before duplicate detection\n",
    "    df_remaining = df.drop(index=variant_indices).copy().drop(columns=['_key_except_year'])\n",
    "\n",
    "    # ----- Complete duplicates (identical across all comparison columns) -----\n",
    "    # comp_cols: use original_cols minus Unique id and Submitted on (same as cols_for_comp)\n",
    "    comp_cols = cols_for_comp.copy()\n",
    "    # Normalize df_remaining for comp_cols (note: df_remaining may be smaller)\n",
    "    df_norm_remaining = normalize_for_compare(df_remaining[comp_cols].astype(object))\n",
    "    df_remaining['_key_all'] = df_norm_remaining.astype(object).apply(lambda row: tuple(row.values), axis=1)\n",
    "\n",
    "    dup_groups = df_remaining.groupby('_key_all')\n",
    "    complete_dup_indices = []\n",
    "    keep_indices = []\n",
    "    for k, g in dup_groups:\n",
    "        if len(g) > 1:\n",
    "            g_sorted = g.sort_values(by='_orig_index')\n",
    "            keep_idx = g_sorted.index[0]\n",
    "            other_idx = g_sorted.index[1:].tolist()\n",
    "            keep_indices.append(keep_idx)\n",
    "            complete_dup_indices.extend(other_idx)\n",
    "        else:\n",
    "            keep_indices.append(g.index[0])\n",
    "\n",
    "    complete_duplicates_df = df_remaining.loc[complete_dup_indices].copy().drop(columns=['_key_all'])\n",
    "    print(f\"Complete duplicate rows found: {len(complete_duplicates_df)}\")\n",
    "\n",
    "    main_df = df_remaining.loc[keep_indices].copy().drop(columns=['_key_all'])\n",
    "\n",
    "    # ----- Use parsed datetime for FY split -----\n",
    "    fy_start = pd.Timestamp('2024-04-01')\n",
    "    fy_end = pd.Timestamp('2025-03-31')\n",
    "\n",
    "    main_df_fy = main_df[(main_df[parsed_col] >= fy_start) & (main_df[parsed_col] <= fy_end)].copy()\n",
    "    main_df_outside = main_df[~((main_df[parsed_col] >= fy_start) & (main_df[parsed_col] <= fy_end))].copy()\n",
    "\n",
    "    print(f\"Rows in Main FY 24-25 (before display fix): {len(main_df_fy)}\")\n",
    "    print(f\"Rows in Main Outside FY (before display fix): {len(main_df_outside)}\")\n",
    "\n",
    "    # Build readable Submitted-on for export: prefer parsed datetime formatted, else raw original\n",
    "    def make_display_col_for_export(df_local):\n",
    "        disp = []\n",
    "        for idx, row in df_local.iterrows():\n",
    "            p = row.get(parsed_col)\n",
    "            if pd.notna(p):\n",
    "                disp.append(pd.to_datetime(p).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "            else:\n",
    "                orig = row.get(submitted_on_col)\n",
    "                if pd.isna(orig):\n",
    "                    disp.append(pd.NA)\n",
    "                else:\n",
    "                    disp.append(str(orig))\n",
    "        return pd.Series(disp, index=df_local.index)\n",
    "\n",
    "    for d in [main_df_fy, main_df_outside, complete_duplicates_df, variant_df]:\n",
    "        d[submitted_on_col] = make_display_col_for_export(d)\n",
    "        # drop helper cols if present\n",
    "        for h in ['_parsed_submitted_on', '_display_submitted_on', '_orig_index']:\n",
    "            if h in d.columns:\n",
    "                d.drop(columns=[h], inplace=True, errors='ignore')\n",
    "\n",
    "    # Prepare safe sheet names\n",
    "    existing = set()\n",
    "    sheet_names_task1 = {\n",
    "        'main_fy': make_safe_sheet_name('Main FY 24-25', existing),\n",
    "        'main_out': make_safe_sheet_name('Main Outside FY', existing),\n",
    "        'dup': make_safe_sheet_name('Complete Duplicates', existing),\n",
    "        'variants': make_safe_sheet_name('YearVariants', existing)\n",
    "    }\n",
    "\n",
    "    print(f\"Writing Task 1 workbook to '{out_task1_path}' ...\")\n",
    "    with pd.ExcelWriter(out_task1_path, engine='xlsxwriter', datetime_format='yyyy-mm-dd') as writer:\n",
    "        main_df_fy.to_excel(writer, sheet_name=sheet_names_task1['main_fy'], index=False)\n",
    "        main_df_outside.to_excel(writer, sheet_name=sheet_names_task1['main_out'], index=False)\n",
    "        complete_duplicates_df.to_excel(writer, sheet_name=sheet_names_task1['dup'], index=False)\n",
    "        variant_df.to_excel(writer, sheet_name=sheet_names_task1['variants'], index=False)\n",
    "    print(\"Task 1 complete.\")\n",
    "\n",
    "    # ---------------- TASK 2: position-wise split BD..BH ----------------\n",
    "    letters = ['BD', 'BE', 'BF', 'BG', 'BH']\n",
    "    cols_to_explode = []\n",
    "    for L in letters:\n",
    "        try:\n",
    "            cname = colname_by_excel_letter(df, L)\n",
    "            cols_to_explode.append(cname)\n",
    "        except Exception:\n",
    "            print(f\"Warning: Column {L} not present (skipping).\")\n",
    "\n",
    "    print(\"Columns to position-wise split:\", cols_to_explode)\n",
    "\n",
    "    existing2 = set()\n",
    "    sheet_names_task2 = {\n",
    "        'fy_exp': make_safe_sheet_name('FY 24-25 Expanded', existing2),\n",
    "        'out_exp': make_safe_sheet_name('Outside FY Expanded', existing2),\n",
    "        'dup_exp': make_safe_sheet_name('Duplicates Expanded', existing2),\n",
    "        'var_exp': make_safe_sheet_name('YearVars Expanded', existing2)\n",
    "    }\n",
    "\n",
    "    print(f\"Creating Task 2 workbook: '{out_task2_path}' ...\")\n",
    "    with pd.ExcelWriter(out_task2_path, engine='xlsxwriter', datetime_format='yyyy-mm-dd') as writer:\n",
    "        expanded_fy = explode_columns_positionwise(main_df_fy.copy(), cols_to_explode)\n",
    "        expanded_fy.to_excel(writer, sheet_name=sheet_names_task2['fy_exp'], index=False)\n",
    "\n",
    "        expanded_out = explode_columns_positionwise(main_df_outside.copy(), cols_to_explode)\n",
    "        expanded_out.to_excel(writer, sheet_name=sheet_names_task2['out_exp'], index=False)\n",
    "\n",
    "        expanded_cd = explode_columns_positionwise(complete_duplicates_df.copy(), cols_to_explode)\n",
    "        expanded_cd.to_excel(writer, sheet_name=sheet_names_task2['dup_exp'], index=False)\n",
    "\n",
    "        expanded_sv = explode_columns_positionwise(variant_df.copy(), cols_to_explode)\n",
    "        expanded_sv.to_excel(writer, sheet_name=sheet_names_task2['var_exp'], index=False)\n",
    "\n",
    "    print(\"Task 2 complete. Files written:\")\n",
    "    print(\" -\", out_task1_path)\n",
    "    print(\" -\", out_task2_path)\n",
    "\n",
    "    # Extra debug: sample some groups that were flagged as duplicates / variants (first few)\n",
    "    if len(complete_duplicates_df) > 0:\n",
    "        print(\"\\nSample complete-duplicate Unique IDs (moved to Complete Duplicates):\")\n",
    "        sn = complete_duplicates_df.head(5)\n",
    "        if unique_col in sn.columns:\n",
    "            print(sn[unique_col].tolist())\n",
    "        else:\n",
    "            print(\"No Unique id column; showing first rows of duplicates:\")\n",
    "            print(sn.head().to_dict(orient='records'))\n",
    "    if len(variant_df) > 0:\n",
    "        print(\"\\nSample Year-Variant Unique IDs (moved to Year Variants):\")\n",
    "        sn = variant_df.head(5)\n",
    "        if unique_col in sn.columns:\n",
    "            print(sn[unique_col].tolist())\n",
    "        else:\n",
    "            print(sn.head().to_dict(orient='records'))\n",
    "\n",
    "\n",
    "# -------------------- DIRECT RUN FOR JUPYTER --------------------\n",
    "input_path = r'D:/Techknowgreen/Rutuja_mam/12noV_Hazardous/Annual_Return_24-25.xlsx'\n",
    "out1 = r'D:/Techknowgreen/Rutuja_mam/12noV_Hazardous/task1_output_C_fix.xlsx'\n",
    "out2 = r'D:/Techknowgreen/Rutuja_mam/12noV_Hazardous/task2_output_C_fix.xlsx'\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
    "    print(\"Starting processing...\")\n",
    "    main(input_path, out1, out2)\n",
    "    print(\"Done.\")\n",
    "except Exception:\n",
    "    print(\"ERROR during processing:\")\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a193b0ac-65c9-4b52-819b-0085d905add7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing...\n",
      "Loading 'D:/Techknowgreen/Rutuja_mam/12noV_Hazardous/Annual_Return_24-25.xlsx' ...\n",
      "Detected Unique ID column: 'Unique id' (will be ignored during duplicate checks).\n",
      "Using Submitted-on column: 'Submited on' (Excel B)\n",
      "Using Submission-Year column: 'Submission year' (Excel AH)\n",
      "Parsed 18310/18310 Submitted-on values.\n",
      "Number of columns used for comparisons (original headers minus Unique id & Submitted on): 62\n",
      "Submission Year Variants rows found: 106\n",
      "Complete duplicate rows found: 5\n",
      "Rows in Main FY 24-25 (before display fix): 8954\n",
      "Rows in Main Outside FY (before display fix): 9245\n",
      "Writing Task 1 workbook to 'D:/Techknowgreen/Rutuja_mam/12noV_Hazardous/task1_output_D_fix.xlsx' ...\n",
      "Task 1 complete.\n",
      "Detected district column (AJ): 'District'\n",
      "Columns to position-wise split: ['Type of waste dispatched', 'Quantity of waste dispatched', 'Unit dispatched', 'Dispatched to', 'Dispatched to facility']\n",
      "Creating Task 2 workbook: 'D:/Techknowgreen/Rutuja_mam/12noV_Hazardous/task2_output_D_fix.xlsx' ...\n",
      "Task 2 complete. Files written:\n",
      " - D:/Techknowgreen/Rutuja_mam/12noV_Hazardous/task1_output_D_fix.xlsx\n",
      " - D:/Techknowgreen/Rutuja_mam/12noV_Hazardous/task2_output_D_fix.xlsx\n",
      "\n",
      "Sample complete-duplicate Unique IDs moved to Complete Duplicates:\n",
      "['MPCB-HW_ANNUAL_RETURN-0000053868', 'MPCB-HW_ANNUAL_RETURN-0000044129', 'MPCB-HW_ANNUAL_RETURN-0000060548', 'MPCB-HW_ANNUAL_RETURN-0000043580', 'MPCB-HW_ANNUAL_RETURN-0000058793']\n",
      "\n",
      "Sample Year-Variant Unique IDs moved to YearVariants:\n",
      "['MPCB-HW_ANNUAL_RETURN-0000051661', 'MPCB-HW_ANNUAL_RETURN-0000051662', 'MPCB-HW_ANNUAL_RETURN-0000060092', 'MPCB-HW_ANNUAL_RETURN-0000060093', 'MPCB-HW_ANNUAL_RETURN-0000043951']\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Jupyter-ready: position-wise expansion with district (AJ) kept on first expanded row\n",
    "and blanked on subsequent expanded rows.\n",
    "\n",
    "Paste the entire cell into Jupyter and run. Edit input_path, out1, out2 at bottom.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl.utils import column_index_from_string\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def find_unique_id_col(cols):\n",
    "    for c in cols:\n",
    "        if isinstance(c, str) and ('unique' in c.lower() and 'id' in c.lower()):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def colname_by_excel_letter(df, letter):\n",
    "    idx = column_index_from_string(letter) - 1\n",
    "    if idx < 0 or idx >= len(df.columns):\n",
    "        raise IndexError(f\"Excel column {letter} -> index {idx} out of range (len cols={len(df.columns)}).\")\n",
    "    return df.columns[idx]\n",
    "\n",
    "def normalize_for_compare(df):\n",
    "    out = df.copy()\n",
    "    for c in out.select_dtypes(include=['object']).columns:\n",
    "        out[c] = out[c].astype(str).str.strip()\n",
    "        out.loc[out[c].isin(['nan', 'None', 'NA', '']), c] = pd.NA\n",
    "    return out\n",
    "\n",
    "def parse_submitted_on_value(val):\n",
    "    raw = val\n",
    "    if pd.isna(val):\n",
    "        return (pd.NaT, None, val)\n",
    "    s = str(val).strip()\n",
    "    if s == '':\n",
    "        return (pd.NaT, None, val)\n",
    "    s = re.sub(r'[\\x00-\\x1f]+', ' ', s).strip()\n",
    "    s = re.sub(r':\\d{1,6}(?=\\s*[AP]M$)', '', s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r'(?i)(\\d{1,2}:\\d{2}:\\d{2})(AM|PM)$', r'\\1 \\2', s)\n",
    "    dt = pd.to_datetime(s, errors='coerce')\n",
    "    if pd.isna(dt):\n",
    "        fmts = [\n",
    "            \"%b %d %Y %I:%M:%S %p\",\n",
    "            \"%d-%b-%Y %H:%M:%S\",\n",
    "            \"%Y-%m-%d %H:%M:%S\",\n",
    "            \"%d/%m/%Y %H:%M:%S\",\n",
    "        ]\n",
    "        for f in fmts:\n",
    "            try:\n",
    "                dt = pd.to_datetime(datetime.strptime(s, f))\n",
    "                break\n",
    "            except Exception:\n",
    "                dt = pd.NaT\n",
    "    display = dt.strftime(\"%Y-%m-%d %H:%M:%S\") if not pd.isna(dt) else s\n",
    "    return (dt, display, raw)\n",
    "\n",
    "def explode_columns_positionwise(df, cols_to_explode, blank_cols=None):\n",
    "    \"\"\"\n",
    "    Position-wise explosion with optional blanking of specific columns in expanded rows,\n",
    "    but preserves blank_cols value on the first generated row (i==0). Subsequent generated\n",
    "    rows (i >= 1) will have blank_cols set to NaN.\n",
    "\n",
    "    Parameters:\n",
    "      df: DataFrame to expand\n",
    "      cols_to_explode: list of column names to split (mapped from BD..BH)\n",
    "      blank_cols: list of column names to blank in subsequent generated rows (not the first)\n",
    "    \"\"\"\n",
    "    out_rows = []\n",
    "    if df.shape[0] == 0:\n",
    "        return pd.DataFrame(columns=df.columns)\n",
    "    blank_cols = blank_cols or []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        splits = {}\n",
    "        max_len = 0\n",
    "        for col in cols_to_explode:\n",
    "            val = row.get(col, \"\")\n",
    "            if pd.isna(val) or str(val).strip() == \"\":\n",
    "                parts = []\n",
    "            else:\n",
    "                parts = [x.strip() for x in str(val).split(',') if x.strip() != \"\"]\n",
    "            splits[col] = parts\n",
    "            max_len = max(max_len, len(parts))\n",
    "\n",
    "        # no splits -> keep original row unchanged (including district)\n",
    "        if max_len == 0:\n",
    "            out_rows.append(row.to_dict())\n",
    "            continue\n",
    "\n",
    "        # create expanded rows; keep blank_cols on first row, blank them on subsequent rows\n",
    "        for i in range(max_len):\n",
    "            new_row = row.copy()\n",
    "            for col in cols_to_explode:\n",
    "                vals = splits[col]\n",
    "                new_row[col] = vals[i] if i < len(vals) else pd.NA\n",
    "            # for i==0 keep original value; for i>=1 set blank_cols to NaN\n",
    "            if i >= 1:\n",
    "                for bc in blank_cols:\n",
    "                    if bc in new_row.index:\n",
    "                        new_row[bc] = pd.NA\n",
    "            out_rows.append(new_row.to_dict())\n",
    "\n",
    "    return pd.DataFrame(out_rows, columns=df.columns)\n",
    "\n",
    "def make_safe_sheet_name(name, existing):\n",
    "    MAX = 31\n",
    "    base = name[:MAX]\n",
    "    candidate = base\n",
    "    i = 1\n",
    "    while candidate in existing:\n",
    "        suffix = f\"_{i}\"\n",
    "        candidate = (base[:MAX - len(suffix)]) + suffix\n",
    "        i += 1\n",
    "        if i > 999:\n",
    "            raise RuntimeError(\"Too many duplicate sheet names\")\n",
    "    existing.add(candidate)\n",
    "    return candidate\n",
    "\n",
    "def main(input_path, out_task1_path, out_task2_path):\n",
    "    print(f\"Loading '{input_path}' ...\")\n",
    "    df = pd.read_excel(input_path, sheet_name=0, header=0, engine='openpyxl')\n",
    "    original_cols = list(df.columns)  # capture original headers immediately\n",
    "\n",
    "    unique_col = find_unique_id_col(original_cols)\n",
    "    if unique_col:\n",
    "        print(f\"Detected Unique ID column: '{unique_col}' (will be ignored during duplicate checks).\")\n",
    "    else:\n",
    "        print(\"No Unique id column detected automatically. All columns (except Submitted on) will be used for comparisons.\")\n",
    "\n",
    "    try:\n",
    "        submitted_on_col = colname_by_excel_letter(df, 'B')\n",
    "        submission_year_col = colname_by_excel_letter(df, 'AH')\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to map Excel letters to columns: {e}\")\n",
    "\n",
    "    print(f\"Using Submitted-on column: '{submitted_on_col}' (Excel B)\")\n",
    "    print(f\"Using Submission-Year column: '{submission_year_col}' (Excel AH)\")\n",
    "\n",
    "    # parse Submitted-on robustly\n",
    "    raw_submitted = df[submitted_on_col].copy()\n",
    "    parsed_list = []\n",
    "    display_list = []\n",
    "    parsed_count = 0\n",
    "    for v in raw_submitted:\n",
    "        dt, disp, raw = parse_submitted_on_value(v)\n",
    "        parsed_list.append(dt)\n",
    "        display_list.append(disp)\n",
    "        if not pd.isna(dt):\n",
    "            parsed_count += 1\n",
    "    print(f\"Parsed {parsed_count}/{len(raw_submitted)} Submitted-on values.\")\n",
    "\n",
    "    parsed_col = '_parsed_submitted_on'\n",
    "    df[parsed_col] = pd.to_datetime(pd.Series(parsed_list))\n",
    "    df['_display_submitted_on'] = display_list\n",
    "\n",
    "    # mark original index for stable selection of \"first\" duplicate\n",
    "    df['_orig_index'] = range(len(df))\n",
    "\n",
    "    # --- Build comparison column lists FROM original_cols so helper cols aren't included ---\n",
    "    exclude_for_comp = [submitted_on_col, '_orig_index']\n",
    "    if unique_col:\n",
    "        exclude_for_comp.append(unique_col)\n",
    "\n",
    "    cols_for_comp = [c for c in original_cols if c not in exclude_for_comp]\n",
    "    print(f\"Number of columns used for comparisons (original headers minus Unique id & Submitted on): {len(cols_for_comp)}\")\n",
    "\n",
    "    df_for_comp_norm = normalize_for_compare(df[cols_for_comp].copy())\n",
    "\n",
    "    # Identify Submission Year Variants\n",
    "    cols_except_year = [c for c in cols_for_comp if c != submission_year_col]\n",
    "    df['_key_except_year'] = df_for_comp_norm[cols_except_year].astype(object).apply(lambda row: tuple(row.values), axis=1)\n",
    "    groups_except_year = df.groupby('_key_except_year')\n",
    "\n",
    "    variant_indices = []\n",
    "    for k, g in groups_except_year:\n",
    "        unique_years = g[submission_year_col].astype(str).dropna().unique()\n",
    "        if len(unique_years) > 1:\n",
    "            variant_indices.extend(g.index.tolist())\n",
    "\n",
    "    variant_df = df.loc[variant_indices].copy().drop(columns=['_key_except_year'])\n",
    "    print(f\"Submission Year Variants rows found: {len(variant_df)}\")\n",
    "\n",
    "    # Remove variants before duplicate detection\n",
    "    df_remaining = df.drop(index=variant_indices).copy().drop(columns=['_key_except_year'])\n",
    "\n",
    "    # Complete duplicates detection\n",
    "    comp_cols = cols_for_comp.copy()\n",
    "    df_norm_remaining = normalize_for_compare(df_remaining[comp_cols].astype(object))\n",
    "    df_remaining['_key_all'] = df_norm_remaining.astype(object).apply(lambda row: tuple(row.values), axis=1)\n",
    "\n",
    "    dup_groups = df_remaining.groupby('_key_all')\n",
    "    complete_dup_indices = []\n",
    "    keep_indices = []\n",
    "    for k, g in dup_groups:\n",
    "        if len(g) > 1:\n",
    "            g_sorted = g.sort_values(by='_orig_index')\n",
    "            keep_idx = g_sorted.index[0]\n",
    "            other_idx = g_sorted.index[1:].tolist()\n",
    "            keep_indices.append(keep_idx)\n",
    "            complete_dup_indices.extend(other_idx)\n",
    "        else:\n",
    "            keep_indices.append(g.index[0])\n",
    "\n",
    "    complete_duplicates_df = df_remaining.loc[complete_dup_indices].copy().drop(columns=['_key_all'])\n",
    "    print(f\"Complete duplicate rows found: {len(complete_duplicates_df)}\")\n",
    "\n",
    "    main_df = df_remaining.loc[keep_indices].copy().drop(columns=['_key_all'])\n",
    "\n",
    "    # FY split using parsed submitted-on\n",
    "    fy_start = pd.Timestamp('2024-04-01')\n",
    "    fy_end = pd.Timestamp('2025-03-31')\n",
    "\n",
    "    main_df_fy = main_df[(main_df[parsed_col] >= fy_start) & (main_df[parsed_col] <= fy_end)].copy()\n",
    "    main_df_outside = main_df[~((main_df[parsed_col] >= fy_start) & (main_df[parsed_col] <= fy_end))].copy()\n",
    "\n",
    "    print(f\"Rows in Main FY 24-25 (before display fix): {len(main_df_fy)}\")\n",
    "    print(f\"Rows in Main Outside FY (before display fix): {len(main_df_outside)}\")\n",
    "\n",
    "    # Prepare readable Submitted-on for export\n",
    "    def make_display_col_for_export(df_local):\n",
    "        disp = []\n",
    "        for idx, row in df_local.iterrows():\n",
    "            p = row.get(parsed_col)\n",
    "            if pd.notna(p):\n",
    "                disp.append(pd.to_datetime(p).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "            else:\n",
    "                orig = row.get(submitted_on_col)\n",
    "                if pd.isna(orig):\n",
    "                    disp.append(pd.NA)\n",
    "                else:\n",
    "                    disp.append(str(orig))\n",
    "        return pd.Series(disp, index=df_local.index)\n",
    "\n",
    "    for d in [main_df_fy, main_df_outside, complete_duplicates_df, variant_df]:\n",
    "        d[submitted_on_col] = make_display_col_for_export(d)\n",
    "        for h in ['_parsed_submitted_on', '_display_submitted_on', '_orig_index']:\n",
    "            if h in d.columns:\n",
    "                d.drop(columns=[h], inplace=True, errors='ignore')\n",
    "\n",
    "    # Write Task 1 with safe sheet names\n",
    "    existing = set()\n",
    "    sheet_names_task1 = {\n",
    "        'main_fy': make_safe_sheet_name('Main FY 24-25', existing),\n",
    "        'main_out': make_safe_sheet_name('Main Outside FY', existing),\n",
    "        'dup': make_safe_sheet_name('Complete Duplicates', existing),\n",
    "        'variants': make_safe_sheet_name('YearVariants', existing)\n",
    "    }\n",
    "\n",
    "    print(f\"Writing Task 1 workbook to '{out_task1_path}' ...\")\n",
    "    with pd.ExcelWriter(out_task1_path, engine='xlsxwriter', datetime_format='yyyy-mm-dd') as writer:\n",
    "        main_df_fy.to_excel(writer, sheet_name=sheet_names_task1['main_fy'], index=False)\n",
    "        main_df_outside.to_excel(writer, sheet_name=sheet_names_task1['main_out'], index=False)\n",
    "        complete_duplicates_df.to_excel(writer, sheet_name=sheet_names_task1['dup'], index=False)\n",
    "        variant_df.to_excel(writer, sheet_name=sheet_names_task1['variants'], index=False)\n",
    "    print(\"Task 1 complete.\")\n",
    "\n",
    "    # Map BD..BH columns and district AJ\n",
    "    letters = ['BD', 'BE', 'BF', 'BG', 'BH']\n",
    "    cols_to_explode = []\n",
    "    for L in letters:\n",
    "        try:\n",
    "            cname = colname_by_excel_letter(df, L)\n",
    "            cols_to_explode.append(cname)\n",
    "        except Exception:\n",
    "            print(f\"Warning: Column {L} not present (skipping).\")\n",
    "\n",
    "    # district column name from AJ (if present)\n",
    "    try:\n",
    "        district_col_name = colname_by_excel_letter(df, 'AJ')\n",
    "        print(f\"Detected district column (AJ): '{district_col_name}'\")\n",
    "    except Exception:\n",
    "        district_col_name = None\n",
    "        print(\"District column AJ not found; expanded rows will not be blanked for district.\")\n",
    "\n",
    "    print(\"Columns to position-wise split:\", cols_to_explode)\n",
    "\n",
    "    # Prepare safe sheet names for Task 2\n",
    "    existing2 = set()\n",
    "    sheet_names_task2 = {\n",
    "        'fy_exp': make_safe_sheet_name('FY 24-25 Expanded', existing2),\n",
    "        'out_exp': make_safe_sheet_name('Outside FY Expanded', existing2),\n",
    "        'dup_exp': make_safe_sheet_name('Duplicates Expanded', existing2),\n",
    "        'var_exp': make_safe_sheet_name('YearVars Expanded', existing2)\n",
    "    }\n",
    "\n",
    "    print(f\"Creating Task 2 workbook: '{out_task2_path}' ...\")\n",
    "    with pd.ExcelWriter(out_task2_path, engine='xlsxwriter', datetime_format='yyyy-mm-dd') as writer:\n",
    "        # pass district_col_name as blank_cols so expanded rows will have district blanked except first generated row\n",
    "        expanded_fy = explode_columns_positionwise(main_df_fy.copy(), cols_to_explode, blank_cols=[district_col_name] if district_col_name else None)\n",
    "        expanded_fy.to_excel(writer, sheet_name=sheet_names_task2['fy_exp'], index=False)\n",
    "\n",
    "        expanded_out = explode_columns_positionwise(main_df_outside.copy(), cols_to_explode, blank_cols=[district_col_name] if district_col_name else None)\n",
    "        expanded_out.to_excel(writer, sheet_name=sheet_names_task2['out_exp'], index=False)\n",
    "\n",
    "        expanded_cd = explode_columns_positionwise(complete_duplicates_df.copy(), cols_to_explode, blank_cols=[district_col_name] if district_col_name else None)\n",
    "        expanded_cd.to_excel(writer, sheet_name=sheet_names_task2['dup_exp'], index=False)\n",
    "\n",
    "        expanded_sv = explode_columns_positionwise(variant_df.copy(), cols_to_explode, blank_cols=[district_col_name] if district_col_name else None)\n",
    "        expanded_sv.to_excel(writer, sheet_name=sheet_names_task2['var_exp'], index=False)\n",
    "\n",
    "    print(\"Task 2 complete. Files written:\")\n",
    "    print(\" -\", out_task1_path)\n",
    "    print(\" -\", out_task2_path)\n",
    "\n",
    "    # debug samples\n",
    "    if len(complete_duplicates_df) > 0:\n",
    "        print(\"\\nSample complete-duplicate Unique IDs moved to Complete Duplicates:\")\n",
    "        sn = complete_duplicates_df.head(5)\n",
    "        if unique_col in sn.columns:\n",
    "            print(sn[unique_col].tolist())\n",
    "        else:\n",
    "            print(sn.head().to_dict(orient='records'))\n",
    "    if len(variant_df) > 0:\n",
    "        print(\"\\nSample Year-Variant Unique IDs moved to YearVariants:\")\n",
    "        sn = variant_df.head(5)\n",
    "        if unique_col in sn.columns:\n",
    "            print(sn[unique_col].tolist())\n",
    "        else:\n",
    "            print(sn.head().to_dict(orient='records'))\n",
    "\n",
    "\n",
    "# -------------------- DIRECT RUN FOR JUPYTER --------------------\n",
    "input_path = r'D:/Techknowgreen/Rutuja_mam/12noV_Hazardous/Annual_Return_24-25.xlsx'\n",
    "out1 = r'D:/Techknowgreen/Rutuja_mam/12noV_Hazardous/task1_output_D_fix.xlsx'\n",
    "out2 = r'D:/Techknowgreen/Rutuja_mam/12noV_Hazardous/task2_output_D_fix.xlsx'\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
    "    print(\"Starting processing...\")\n",
    "    main(input_path, out1, out2)\n",
    "    print(\"Done.\")\n",
    "except Exception:\n",
    "    print(\"ERROR during processing:\")\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9a3275-12fc-4e6d-880c-14867713ab4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efa2422-2d48-4379-bd22-9b9ed434cbb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
